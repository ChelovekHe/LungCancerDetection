{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel based on work of [Tan, M. et al][1] approach which imply clusters generation via the minima of divergence of normalized gradient taken over CT scan and merged them with clasters generated by dot and line 3D [selective enhancement filters][2] \n",
    "\n",
    "\n",
    "  [1]: http://dx.doi.org/10.1118/1.3633941\n",
    "  [2]: http://dx.doi.org/10.1118/1.1581411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.ndimage import morphology \n",
    "from tqdm import tqdm\n",
    "from os.path import join, basename, isfile\n",
    "from scipy.ndimage import label\n",
    "import scipy.ndimage.filters as filters\n",
    "import scipy\n",
    "from glob import glob\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from skimage import measure\n",
    "\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter, laplace\n",
    "import dicom\n",
    "from scipy.linalg import norm\n",
    "import pandas as pd\n",
    "\n",
    "CPU = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next five cells was taken from [Full Preprocessing Tutorial][1] by [Guido Zuidhof][2]\n",
    "\n",
    "\n",
    "  [1]: https://www.kaggle.com/gzuidhof/data-science-bowl-2017/full-preprocessing-tutorial\n",
    "  [2]: https://www.kaggle.com/gzuidhof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resample(image, scan, new_spacing=[1,1,1]):\n",
    "    # Determine current pixel spacing\n",
    "    spacing = np.array([scan[0].SliceThickness] + scan[0].PixelSpacing, dtype=np.float32)\n",
    "\n",
    "    resize_factor = spacing / new_spacing\n",
    "    new_real_shape = image.shape * resize_factor\n",
    "    new_shape = np.round(new_real_shape)\n",
    "    real_resize_factor = new_shape / image.shape\n",
    "    new_spacing = spacing / real_resize_factor\n",
    "    \n",
    "    image = scipy.ndimage.interpolation.zoom(image, real_resize_factor, mode='nearest')\n",
    "    \n",
    "    return image, new_spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the scans in given folder path\n",
    "def load_scan(path):\n",
    "    slices = [dicom.read_file(join(path, pslice)) \n",
    "              for pslice in glob(join(path, '*.dcm'))]\n",
    "    slices.sort(key = lambda x: float(x.ImagePositionPatient[2]))\n",
    "    try:\n",
    "        slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])\n",
    "    except:\n",
    "        slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation)\n",
    "        \n",
    "    for s in slices:\n",
    "        s.SliceThickness = slice_thickness\n",
    "        \n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pixels_hu(slices):\n",
    "    image = np.stack([s.pixel_array for s in slices])\n",
    "    # Convert to int16 (from sometimes int16), \n",
    "    # should be possible as values should always be low enough (<32k)\n",
    "    image = image.astype(np.int16)\n",
    "\n",
    "    # Set outside-of-scan pixels to 0\n",
    "    # The intercept is usually -1024, so air is approximately 0\n",
    "    image[image == -2000] = 0\n",
    "    \n",
    "    # Convert to Hounsfield units (HU)\n",
    "    for slice_number in range(len(slices)):\n",
    "        \n",
    "        intercept = slices[slice_number].RescaleIntercept\n",
    "        slope = slices[slice_number].RescaleSlope\n",
    "        \n",
    "        if slope != 1:\n",
    "            image[slice_number] = slope * image[slice_number].astype(np.float64)\n",
    "            image[slice_number] = image[slice_number].astype(np.int16)\n",
    "            \n",
    "        image[slice_number] += np.int16(intercept)\n",
    "    \n",
    "    return np.array(image, dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def largest_label_volume(im, bg=-1):\n",
    "    vals, counts = np.unique(im, return_counts=True)\n",
    "\n",
    "    counts = counts[vals != bg]\n",
    "    vals = vals[vals != bg]\n",
    "\n",
    "    if len(counts) > 0:\n",
    "        return vals[np.argmax(counts)]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segment_lung_mask(image, fill_lung_structures=True):\n",
    "    \n",
    "    # not actually binary, but 1 and 2. \n",
    "    # 0 is treated as background, which we do not want\n",
    "    binary_image = np.array(image > -320, dtype=np.int8)+1\n",
    "    labels = measure.label(binary_image)\n",
    "    \n",
    "    # Pick the pixel in the very corner to determine which label is air.\n",
    "    #   Improvement: Pick multiple background labels from around the patient\n",
    "    #   More resistant to \"trays\" on which the patient lays cutting the air \n",
    "    #   around the person in half\n",
    "    background_label = labels[0,0,0]\n",
    "    \n",
    "    #Fill the air around the person\n",
    "    binary_image[background_label == labels] = 2\n",
    "    \n",
    "    \n",
    "    # Method of filling the lung structures (that is superior to something like \n",
    "    # morphological closing)\n",
    "    if fill_lung_structures:\n",
    "        # For every slice we determine the largest solid structure\n",
    "        for i, axial_slice in enumerate(binary_image):\n",
    "            axial_slice = axial_slice - 1\n",
    "            labeling = measure.label(axial_slice)\n",
    "            l_max = largest_label_volume(labeling, bg=0)\n",
    "            \n",
    "            if l_max is not None: #This slice contains some lung\n",
    "                binary_image[i][labeling != l_max] = 1\n",
    "\n",
    "    \n",
    "    binary_image -= 1 #Make the image actual binary\n",
    "    binary_image = 1-binary_image # Invert it, lungs are now 1\n",
    "    \n",
    "    # Remove other air pockets insided body\n",
    "    labels = measure.label(binary_image, background=0)\n",
    "    l_max = largest_label_volume(labels, bg=0)\n",
    "    if l_max is not None: # There are air pockets\n",
    "        binary_image[labels != l_max] = 0\n",
    " \n",
    "    return binary_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):    \n",
    "    ct_scan = load_scan(path)\n",
    "    patient = get_pixels_hu(ct_scan)\n",
    "    patient, spacing = resample(patient, ct_scan, SPACING)\n",
    "    \n",
    "    mask = segment_lung_mask(patient)\n",
    "    mask = morphology.binary_fill_holes(\n",
    "        morphology.binary_dilation(\n",
    "            morphology.binary_fill_holes(mask > 0), \n",
    "            iterations=4)\n",
    "    )\n",
    "\n",
    "    return patient, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants presented in papers\n",
    "SPACING = array([1., 1., 1.])\n",
    "ISOLATED_THRESHOLD = -600\n",
    "DOT_ENHANCED_THRESHOLD = 6\n",
    "FILTERS_AMOUNT = 6\n",
    "ISOLATED_MIN_VOLUME = 9\n",
    "ISOLATED_MAX_VOLUME = 500\n",
    "JUXTAVASCULAR_MIN_VOLUME = 9\n",
    "JUXTAPLEURAL_MIN_VALUME = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmas calculation for enhanced  nodule and vessels of different sizes   \n",
    "It should be noted that the relationship  between the smoothing scales is exponential [\\[here\\]][1]. \n",
    "\n",
    "\n",
    "  [1]: http://link.springer.com/article/10.1007/BF00336961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_RADIUS = 4\n",
    "MAX_RADIUS = 16\n",
    "\n",
    "def get_scales(bottom=MIN_RADIUS, top=MAX_RADIUS, \n",
    "               amount=FILTERS_AMOUNT):\n",
    "    radius = (top / bottom) ** (1. / (amount - 1))\n",
    "    sigmas = [bottom / 4.]\n",
    "    for i in range(amount - 1):\n",
    "        sigmas.append(sigmas[0] * (radius ** i + 1))\n",
    "    return sigmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhanced filters based on hessian propreties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hessian(field, coords):\n",
    "    grad = gradient(field)\n",
    "    axis = [[0, 1, 2], [1, 2], [2]]\n",
    "    hess = [gradient(deriv, axis=j) \n",
    "            for i, deriv in enumerate(grad) \n",
    "            for j in axis[i]]\n",
    "\n",
    "#   [(0, xx), (1, xy), (2, xz), (3, yy), (4, yz), (5, zz)]\n",
    "#   x, y, z -> 3, 3, x, y, z -> 3, 3, N\n",
    "\n",
    "    for j in range(len(hess)):\n",
    "        hess[j] = hess[j][coords]\n",
    "\n",
    "    return asarray([[hess[0], hess[1], hess[2]],\n",
    "                    [hess[1], hess[3], hess[4]],\n",
    "                    [hess[2], hess[4], hess[5]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enhanced_filter(patient, coords, sigma):\n",
    "    filtered = gaussian_filter(patient, sigma=sigma)\n",
    "    hess = hessian(filtered, coords=coords)\n",
    "    hess = [hess[:, :, i] for i in range(hess.shape[-1])]\n",
    "    with Pool(CPU) as pool:\n",
    "        eigs = pool.map(linalg.eigvalsh, \n",
    "                        hess)\n",
    "\n",
    "    sigma_sqr = sigma ** 2\n",
    "    z_dot = [sigma_sqr * (eig_val[2] ** 2) / abs(eig_val[0]) \n",
    "             if eig_val[0] < 0 \n",
    "             and eig_val[1] < 0 \n",
    "             and eig_val[2] < 0 \n",
    "             else 0\n",
    "             for eig_val in eigs]\n",
    "\n",
    "    z_line = [sigma_sqr * abs(eig_val[1]) \n",
    "              * (abs(eig_val[1]) - abs(eig_val[2])) \n",
    "              / abs(eig_val[0]) \n",
    "              if eig_val[0] < 0 \n",
    "              and eig_val[1] < 0 \n",
    "              else 0\n",
    "              for eig_val in eigs]\n",
    "    return z_dot, z_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_enhs_filters(patient, mask, include_plane=False):\n",
    "    sigmas = get_scales()\n",
    "    enh_dot = zeros(mask.shape)\n",
    "    enh_line = zeros(mask.shape)\n",
    "    coords = where(mask)\n",
    "    \n",
    "    z_dot = list()\n",
    "    z_line = list()\n",
    "    for sigma in sigmas:\n",
    "        dot, line = enhanced_filter(patient, coords, sigma)\n",
    "        z_dot.append(dot)\n",
    "        z_line.append(line)\n",
    "\n",
    "\n",
    "    enh_dot[coords] = asarray(z_dot).max(axis=0)\n",
    "    enh_line[coords] = asarray(z_line).max(axis=0)\n",
    "\n",
    "    return enh_dot, enh_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of clastering via maxima divergence of normalized gradient (DNG) was shown above, but it will not going to be used further due to following two situations: 1) Why they called it maxima of DNG, instead of minima? Nodule is a concave function hence it's second derivative need to be negative. Though it may be just  a typo, by they focused attention on this.   2) Provided thresholds in a paper did not match with output values of maxima of sign changed DNG, this ofcourse may be still my mistakes, so I'll be glad if you point me on 'em."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def div_of_norm_grad(sigma, patient):\n",
    "    grad = asarray(gradient(patient))\n",
    "    grad /= norm(grad, axis=0) + 1e-3 # Smooth const\n",
    "    grad = [gaussian_filter(deriv, sigma=sigma) for deriv in grad]\n",
    "    return sum([gradient(el, axis=i) \n",
    "                for i, el in enumerate(grad)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxima_divergence(masks_pats):\n",
    "    with Pool(CPU) as pool:\n",
    "        divs = pool.map(\n",
    "            functools.partial(divergence, \n",
    "                              patient=pat), \n",
    "            sigmas\n",
    "        )\n",
    "        divs = -1 * asarray(divs) * mask \n",
    "        divs = divs.max(axis=0)\n",
    "        divs_list.append(divs.copy())\n",
    "    return divs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clastering generation was done via merging enhanced and original thresholded volumetric images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_in(colour, labe, dng_colours):\n",
    "    if colour in dng_colours:\n",
    "        return labe == colour\n",
    "\n",
    "\n",
    "def get_pure_isol(patient, mask, enh_dot):\n",
    "    isolated = (patient > -600) * (mask > 0) * (enh_dot < 6) \n",
    "    labe, iso_nodules_num = label(isolated)\n",
    "    volumes = bincount(labe.flatten())\n",
    "    colours = where((volumes > ISOLATED_MIN_VOLUME) \n",
    "                & (volumes < ISOLATED_MAX_VOLUME))[0]\n",
    "    \n",
    "    isolated = zeros(isolated.shape).astype(bool)\n",
    "    for colour in colours:\n",
    "        isolated |= labe == colour\n",
    "        \n",
    "    return isolated, iso_nodules_num\n",
    "\n",
    "\n",
    "def get_pure_j_va(patient, mask, enh_line, iso):\n",
    "    juxtavascular = (patient > -600) * (mask > 0) * (enh_line > 150)\n",
    "    j_va_candidates = (1 - juxtavascular) * (1 - iso)\n",
    "    labe, j_va_nodules_num = label(j_va_candidates)\n",
    "\n",
    "    volumes = bincount(labe.flatten())\n",
    "    colours = where((volumes > JUXTAVASCULAR_MIN_VOLUME) \n",
    "                    & (volumes < ISOLATED_MAX_VOLUME))[0]\n",
    "    j_va = zeros(juxtavascular.shape).astype(bool)\n",
    "    for colour in colours:\n",
    "        j_va |= labe == colour\n",
    "    \n",
    "    return j_va, j_va_nodules_num\n",
    "\n",
    "\n",
    "def get_pure_j_pl(patient, mask, enh_dot):\n",
    "    fixed_mask = morphology.binary_erosion(mask > 0,iterations=4)\n",
    "    border_mask = fixed_mask * (1 - morphology.binary_erosion(fixed_mask > 0,iterations=4))\n",
    "    juxtapleural = (patient > -400) * (border_mask > 0) * (enh_dot > 4)\n",
    "\n",
    "    labe, j_pl_num = label(juxtapleural)\n",
    "    volumes = bincount(labe.flatten())\n",
    "    colours = where((volumes > JUXTAPLEURAL_MIN_VALUME) \n",
    "                    & (volumes < ISOLATED_MAX_VOLUME))[0]\n",
    "    j_pl = zeros(juxtapleural.shape).astype(bool)\n",
    "    for colour in colours:\n",
    "        j_pl |= labe == colour\n",
    "    return j_pl, j_pl_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pure_nodules(patient, mask, enh):\n",
    "    \"\"\"\n",
    "    Here: \n",
    "    1 is for isolated\n",
    "    2 is for j_va\n",
    "    4 is for j_pl\n",
    "    \"\"\"\n",
    "    iso, iso_num = get_pure_isol(patient, mask, enh[0])\n",
    "    j_va, j_va_num = get_pure_j_va(patient, mask, enh[1], iso)\n",
    "    j_pl, j_pl_num = get_pure_j_pl(patient, mask, enh[0])\n",
    "    return 2 * j_va + iso + 4 * j_pl, (iso_num, j_va_num, j_pl_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It tooks around 500s  and resulted on average 730 candidates  per patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def operate(path):\n",
    "    patient, mask = load_data(path)\n",
    "    enhs = apply_enhs_filters(patient, mask, \n",
    "                              include_plane=False)\n",
    "    nodules, amounts = get_pure_nodules(patient, mask, enhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the Kaggle kernel limitation I post the results in this form with candidates in center of the images:\n",
    " ![Due to the Kaggle kernel limitation I post the results in this form][1]\n",
    "\n",
    "\n",
    "  [1]: https://s30.postimg.org/6lnslbdox/index.png"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
